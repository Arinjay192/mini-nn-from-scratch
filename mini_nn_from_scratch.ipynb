{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "9f113a05-579b-4a97-a128-f998405bd87c",
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ndef load_iris_local(file_path=\"iris.csv\", test_size=0.2, random_state=42):\n    \"\"\"\n    Returns  X_train, X_dev, y_train, y_dev\n    Shapes:   (n_features, m_samples)\n    \"\"\"\n    df = pd.read_csv(file_path)\n    y  = df[\"species\"].factorize()[0]     # map species strings → 0,1,2\n    X  = df.iloc[:, :4].values            # first 4 columns\n\n    X = StandardScaler().fit_transform(X)\n\n    X_train, X_dev, y_train, y_dev = train_test_split(\n        X, y, test_size=test_size, random_state=random_state, stratify=y\n    )\n    return X_train.T, X_dev.T, y_train, y_dev\n\nX_train, X_dev, y_train, y_dev = load_iris_local()\nprint(\"train shape:\", X_train.shape, \"| dev shape:\", X_dev.shape)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "train shape: (4, 120) | dev shape: (4, 30)\n"
        }
      ],
      "execution_count": 5
    },
    {
      "id": "663c3ee7-2c8f-4764-bd52-5a26f48b00f5",
      "cell_type": "code",
      "source": "\ndef relu(Z):\n    return np.maximum(0, Z)\n\ndef relu_deriv(Z):\n    return (Z > 0).astype(float)\n\ndef softmax(Z):\n    exp_Z = np.exp(Z - Z.max(axis=0, keepdims=True))  # numerical-stability trick\n    return exp_Z / exp_Z.sum(axis=0, keepdims=True)\n\ndef one_hot(y, num_classes=3):\n    m  = y.size\n    oh = np.zeros((num_classes, m))\n    oh[y, np.arange(m)] = 1\n    return oh\n\ndef accuracy(preds, y):\n    return np.mean(preds == y) * 100\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "e71d8ed4-01b4-4ac5-b1fc-33fa9f3d475f",
      "cell_type": "code",
      "source": "def init_params(n_x, n_h, n_y, seed=42):\n    rng = np.random.default_rng(seed)\n    W1 = rng.uniform(-0.5, 0.5, size=(n_h, n_x))\n    b1 = np.zeros((n_h, 1))\n    W2 = rng.uniform(-0.5, 0.5, size=(n_y, n_h))\n    b2 = np.zeros((n_y, 1))\n    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "id": "27838ed4-4a6e-4111-89b3-3f6d0f18dc00",
      "cell_type": "code",
      "source": "def forward_prop(params, X):\n    W1, b1, W2, b2 = params[\"W1\"], params[\"b1\"], params[\"W2\"], params[\"b2\"]\n    Z1 = W1 @ X + b1\n    A1 = relu(Z1)\n    Z2 = W2 @ A1 + b2\n    A2 = softmax(Z2)\n    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n    return A2, cache\n\ndef backward_prop(params, cache, X, Y):\n    m   = X.shape[1]\n    W2  = params[\"W2\"]\n    A1, A2 = cache[\"A1\"], cache[\"A2\"]\n\n    dZ2 = A2 - Y                       # (n_y, m)\n    dW2 = (1/m) * dZ2 @ A1.T\n    db2 = (1/m) * dZ2.sum(axis=1, keepdims=True)\n\n    dZ1 = W2.T @ dZ2 * relu_deriv(cache[\"Z1\"])\n    dW1 = (1/m) * dZ1 @ X.T\n    db1 = (1/m) * dZ1.sum(axis=1, keepdims=True)\n\n    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n    return grads\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "id": "cef53c62-abdd-4655-942d-c480a9ab1332",
      "cell_type": "code",
      "source": "def update_params(params, grads, lr):\n    params[\"W1\"] -= lr * grads[\"dW1\"]\n    params[\"b1\"] -= lr * grads[\"db1\"]\n    params[\"W2\"] -= lr * grads[\"dW2\"]\n    params[\"b2\"] -= lr * grads[\"db2\"]\n    return params\n\ndef train_nn(X_train, y_train, X_dev, y_dev,\n             n_h=10, iterations=1000, lr=0.05, print_every=100):\n    \n    n_x, n_y = X_train.shape[0], len(np.unique(y_train))\n    params   = init_params(n_x, n_h, n_y)\n    Y_train  = one_hot(y_train, n_y)\n    Y_dev    = one_hot(y_dev,   n_y)\n\n    for i in range(1, iterations + 1):\n        A2, cache = forward_prop(params, X_train)\n        grads     = backward_prop(params, cache, X_train, Y_train)\n        params    = update_params(params, grads, lr)\n\n        if i % print_every == 0 or i == 1:\n            train_pred = np.argmax(A2, axis=0)\n            dev_pred   = np.argmax(forward_prop(params, X_dev)[0], axis=0)\n            print(f\"Iter {i:4d} | \"\n                  f\"train acc: {accuracy(train_pred, y_train):5.2f}% | \"\n                  f\"dev acc: {accuracy(dev_pred, y_dev):5.2f}%\")\n\n    return params\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "id": "55d59528-7239-4e5f-8a26-975940135ff9",
      "cell_type": "code",
      "source": "params = train_nn(X_train, y_train, X_dev, y_dev,\n                  n_h=10, iterations=1000, lr=0.05)\n\n# Test on one dev sample\nidx        = 0\nsample     = X_dev[:, idx:idx+1]               # keep 2-D\ntrue_label = y_dev[idx]\npred_label = np.argmax(forward_prop(params, sample)[0])\n\nprint(f\"\\nSample {idx}: model ⇒ {pred_label}, true ⇒ {true_label}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Iter    1 | train acc: 15.00% | dev acc: 16.67%\nIter  100 | train acc: 89.17% | dev acc: 80.00%\nIter  200 | train acc: 93.33% | dev acc: 90.00%\nIter  300 | train acc: 95.00% | dev acc: 90.00%\nIter  400 | train acc: 95.83% | dev acc: 96.67%\nIter  500 | train acc: 96.67% | dev acc: 96.67%\nIter  600 | train acc: 96.67% | dev acc: 96.67%\nIter  700 | train acc: 97.50% | dev acc: 96.67%\nIter  800 | train acc: 97.50% | dev acc: 96.67%\nIter  900 | train acc: 97.50% | dev acc: 96.67%\nIter 1000 | train acc: 97.50% | dev acc: 96.67%\n\nSample 0: model ⇒ 0, true ⇒ 0\n"
        }
      ],
      "execution_count": 10
    },
    {
      "id": "0a045455-62a6-412e-ba57-8607559f1675",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}